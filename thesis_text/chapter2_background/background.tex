\chapter{Background} 
\label{chapter:background}
Information retrieval (IR) is the activity of obtaining information resources which are relevant to a given query. In terms of semantic similarity, the task of information retrieval is to find documents which are semantically similar to a given query. One of the easiest approaches for finding relevant documents given a query is TFIDF.
TFIDF, short for term frequency inverse document frequency, is an information retrieval technique that shows how important a word is to a document. Another information retrieval algorithm known as BM25 (BM stands for Best Match) \cite{robertson2009probabilistic} can be used to retrieve matching documents according to their relevance to a given query. \cite{dumais2004latent} introduced a word embedding method as an extension of TDIDF known as Latent Semantic Analysis (LSA). \cite{mikolov2013efficient}, \cite{pennington2014glove} and \cite{shazeer2016swivel} have introduced neural network based word embedding models which have now become benchmarks in semantic similarity extraction.


\section{Summary of relevant approaches}
The most naive and intuitive way of finding relevant documents given a query term is to use term frequency (TF). Term frequency assumes that the more frequently the given query term appears in a document the more relevant that document is to the given query. Term frequency, however, suffers from a critical problem: all words are considered equally important when it comes to assessing relevancy on a query. In fact, though some words appear multiple times in a document they have very little discriminating power  in determining the relevancy. For instance, a collection of documents on football is likely to have the word football or soccer in almost every document. To circumvent this issue a technique known as inverse document frequency (IDF) is used to  attenuate the effect of words that occur too often in the collection of documents to be meaningful for relevance determination.
We define the inverse frequency of a word $w$ as follows:

\begin{displaymath}
\mbox{IDF}_w = \log {N\over \mbox{DF}_w}.
\end{displaymath}

where $DF_{w}$ is the document frequency and is defined as the number of documents in the collection that contain a word $w$ and N is the total number of documents. Thus the inverse document frequency of a rare word is high, whereas the inverse document frequency of a frequent word is likely to be low. Each word has its own term frequency and inverse document frequency score, the product of the two scores is called the TFIDF weight of that word. \citet{ramos2003using} provide evidence that TFIDF returns
documents highly correlated to the given query. Different weight schemes for these counts lead to a variety of TFIDF ranking features. One very successful TFIDF
formulation is known as BM25 \citep{robertson2009probabilistic}.
BM25  is a bag-of-words information retrieval function that ranks a set of documents based on their relevance to the query terms. Tweaking different components and parameters produce different variations of the BM25. \citet{mitra2016dual} have shown BM25 to be effective in information retrieval and have also proposed using it in an ensemble model along with another embedding model, namely Word2Vec. 


<<<<<<< Updated upstream
Another extension introduced by \citet{dumais2004latent} is known as Latent Semantic Analysis(LSA). It uses Singular Value Decomposition (SVD) to perform dimensionality reduction on the TFIDF vectors resulting in smaller and better features. In Latent Semantic Analysis documents are represented as bags-of-words, where the order of the words in a document is not important, only how many times each word appears in a document. Furthermore, it assumes that words which are close in meaning will appear in similar pieces of text, a concept known as the distributional hypothesis. \citet{boling2014semantic} have shown that Latent Semantic Analysis performs very well in finding semantic similarity between documents. Other popular models which use distributional hypothesis are Word2Vec \citep{mikolov2013efficient} and Swivel \citep{shazeer2016swivel}.
=======
Another extension introduced by \citet{dumais2004latent} is known as Latent Semantic Analysis(LSA). It uses Singular Value Decomposition (SVD) to perform dimensionality reduction on the TFIDF vectors resulting in smaller and better features. In Latent Semantic Analysis documents are represented as bags-of-words, where the order of the words in a document is not important, only how many times each word appears in a document. Furthermore, It assumes that words which are close in meaning will appear in similar pieces of text, a concept known as the distributional hypothesis. \citet{boling2014semantic} have shown that Latent Semantic Analysis performs very well in finding semantic similarity between documents. Other popular models which use distributional hypothesis are Word2Vec \citep{mikolov2013efficient} and Swivel \citep{shazeer2016swivel}.
>>>>>>> Stashed changes
\subsection{Word and Document Embeddings} 
Word embedding is a language modeling and feature learning technique in natural language processing(NLP) which maps words and phrases to the real number vector space of the desired dimension. 

\paragraph{Word2Vec} Word2Vec model was  introduced by \citet{mikolov2013efficient}. It uses distributed vector representation of words, a well-known framework for learning word vectors as shown in the Figure \ref{fig:Word2Vec model}. The task is to learn to predict a word given other words in the context.
More formally, given a sequence of training words
$w_{1}, w_{2}, w_{3}, ..., w_{T} $, the objective of the word vector model is to maximize the average log probability
\\
\begin{equation}
\frac{1}{T} \sum_{t=K}^{T-K} \log p(w_{t} \mid w_{t-1},....,w_{t+1}) 
\end{equation}

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm, height=5cm]{w2v.png}
	\caption{A framework for learning word vectors. Context of
		three words (“the,” “cat,” and “sat”) is used to predict the fourth
		word (“on”). The input words are mapped to columns of the matrix
		W to predict the output word.}
	\label{fig:Word2Vec model}
\end{figure}


 \citet{bojanowski2016enriching} propose an extention of Word2Vec model known as FastText. It learns word
 representations while taking into account morphology.
 FastText models morphology by considering subword
 units, and representing words by a sum of its character
 n-grams.  Since FastText exploits subword information, it can also
 compute valid representations for out-of-vocabulary
 words. FastText obtains representations for out-of-vocabulary words by summing the vectors of character
 n-grams.
  
 \citet{ghosh2016characterizing} introduce a vocabulary driven Word2Vec method known as Dis2Vec which is
 used to generate disease specific word embeddings from unstructured health
 related news corpus. The input corpus D consists of a collection of word context pairs. Based on the vocabulary $V$, we can categorize the word context pairs into three types as shown
 below:
 \\
 \begin{itemize}
 	\item $ D(d) = {(w, c): w \in V ∧c \in V }$, i.e. both the word w and the context c are in V
 	\item $D(\rightharpoondown d) = {(w, c): w \notin V ∧c \notin V }$, i.e. neither the word w nor the context c are in V
 	\item $D(d)(\rightharpoondown d) = {(w, c): w \in V \oplus c \in V }$, i.e. either the word w is in V or the context c is in V but both cannot be in V
 \end{itemize}
 
 Each of these categories of (w, c) pairs
 needs special consideration while generating disease specific embeddings.
 
 
 All the above mentioned word embedding models learn word embeddings from co-occurrence information in corpora.
 One drawback of learning word embeddings by this approach is that such methods will generally fail to tell synonyms from
 antonyms \citep{mohammad2008computing}. For example, words like east and west
 or expensive and inexpensive appear in near-identical contexts, which means
 that distributional models produce very similar word vectors for such words. Such embedding is very undesirable when the goal is to find semantic similarity between documents. 
 \citet{mrksic:2016:naacl} proposed a novel counter-fitting method which injects antonym and
 synonymy constraints into vector space representations in order to circumvent this issue. Table \ref{tab:counter_fitting} shows the results \citet{mrksic:2016:naacl} achieved using their counter-fitting technique.
 
 \begin{table}[h]
 	\begin{center}
 		\begin{tabular}{ c c c c } 
 			\hline
 			& east & expensive & British \\
 			\hline
 			\multirow{5}{4em}{Before} &
 			
 			west & pricey &  American
 			\\ 
 			& north & cheaper & Australian\\ 
 			& south & costly & Britain\\ 
 			& southeast & overpriced & European\\
 			& northeast & inexpensive & England\\
 			\hline
 			\multirow{5}{4em}{After} & 
 			eastward & costly & Brits\\ 
 			& eastern & pricy & London\\ 
 			& easterly & overpriced & BBC\\ 
 			& - & pricey & UK\\ 
 			& - & afford & Britain\\ 
 			\hline
 		\end{tabular}
 		
 	\end{center}
 	
 	\caption{Nearest neighbours for target words using GloVe
 		vectors before and after counter-fitting} \label{tab:counter_fitting}
 \end{table}
 
 An alternative to
 the bag-of-words approach is to derive contexts
 based on the syntactic relations the word participates
 in as proposed by \citet{levy2014dependency}.

All the above mentioned approaches are word embedding models and do not generalize to sentences and documents. \citet{jsnior2017nilc} propose two methods for obtaining sentence and document level embeddings. The first approach obtains vector embeddings for documents by averaging the word embeddings of all the words in a document. The second approach also averages word embeddings, but each embedding vector is now weighted (multiplied) by the TFIDF
of the word it represents.
\paragraph{Doc2Vec} An extension to the Word2Vec model known as Doc2Vec was introduced by \citet{le2014distributed}. Doc2Vec is capable of constructing representations of input sequences of
variable length. Unlike some of the previous approaches, it is general and
applicable to texts of any length: sentences, paragraphs, and documents. In
Doc2Vec framework (see Figure \ref{fig:doc2vec model}), every document is mapped to a unique
vector, and every word is also mapped to a unique vector. The document
vector and word vectors are averaged or concatenated to predict the next
word in a context. The only difference to a Word2Vec model is the additional
document token. It acts as a memory that remembers what is missing from
the current context or the topic of the document. The document vectors and
word vectors are trained using stochastic gradient descent and the gradient
is obtained via back-propagation.

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm, height=5cm]{para}
	\caption[]{A framework for learning paragraph vector. This framework
		is similar to the framework presented in Figure 1; the only
		change is the additional paragraph token that is mapped to a vector
		via matrix D.}
	\label{fig:doc2vec model}
\end{figure}




	