%% LaTeX2e file `Proposal.bib'
%% generated by the `filecontents' environment
%% from source `Proposal' on 2018/02/19.
%%
@misc{pubmed,
  author = {National Center for Biotechnology Information},
  title = {PubMed},
  howpublished = {\url{https://www.ncbi.nlm.nih.gov/pubmed}}
}

@misc{annoy,
 author = {Github},
 title = {Annoy},
 howpublished = {\url{https://github.com/spotify/annoy}}
}



@misc{selenium,
 author = {Selenium Web driver},
 title = {Selenium},
 howpublished = {\url{http://www.seleniumhq.org/docs/03_webdriver.jsp }}
}
@inproceedings{le2014distributed,
 title={Distributed representations of sentences and documents},
 author={Le, Quoc and Mikolov, Tomas},
 booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
 pages={1188--1196},
 year={2014}
}




@Inbook{Bengio2006,
 author="Bengio, Yoshua
 and Schwenk, Holger
 and Sen{\'e}cal, Jean-S{\'e}bastien
 and Morin, Fr{\'e}deric
 and Gauvain, Jean-Luc",
 editor="Holmes, Dawn E.
 and Jain, Lakhmi C.",
 title="Neural Probabilistic Language Models",
 bookTitle="Innovations in Machine Learning: Theory and Applications",
 year="2006",
 publisher="Springer Berlin Heidelberg",
 address="Berlin, Heidelberg",
 pages="137--186",
 abstract="A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.",
 isbn="978-3-540-33486-6",
 doi="10.1007/3-540-33486-6_6",
 url="https://doi.org/10.1007/3-540-33486-6_6"
}

@article{mikolov2013efficient,
 title={Efficient estimation of word representations in vector space},
 author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 journal={arXiv preprint arXiv:1301.3781},
 year={2013}
}


@inproceedings{pennington2014glove,
 title={Glove: Global vectors for word representation},
 author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
 booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
 pages={1532--1543},
 year={2014}
}

@article{robertson2009probabilistic,
 title={The probabilistic relevance framework: BM25 and beyond},
 author={Robertson, Stephen and Zaragoza, Hugo and others},
 journal={Foundations and Trends{\textregistered} in Information Retrieval},
 volume={3},
 number={4},
 pages={333--389},
 year={2009},
 publisher={Now Publishers, Inc.}
}


@article{chen2017efficient,
 title={Efficient vector representation for documents through corruption},
 author={Chen, Minmin},
 journal={arXiv preprint arXiv:1707.02377},
 year={2017}
}

@article{collobert2011natural,
 title={Natural language processing (almost) from scratch},
 author={Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
 journal={Journal of Machine Learning Research},
 volume={12},
 number={Aug},
 pages={2493--2537},
 year={2011}
}


@article{mitchell2010composition,
 title={Composition in distributional models of semantics},
 author={Mitchell, Jeff and Lapata, Mirella},
 journal={Cognitive science},
 volume={34},
 number={8},
 pages={1388--1429},
 year={2010},
 publisher={Wiley Online Library}
}


@inproceedings{zanzotto2010estimating,
 title={Estimating linear models for compositional distributional semantics},
 author={Zanzotto, Fabio Massimo and Korkontzelos, Ioannis and Fallucchi, Francesca and Manandhar, Suresh},
 booktitle={Proceedings of the 23rd International Conference on Computational Linguistics},
 pages={1263--1271},
 year={2010},
 organization={Association for Computational Linguistics}
}

@inproceedings{yessenalina2011compositional,
 title={Compositional matrix-space models for sentiment analysis},
 author={Yessenalina, Ainur and Cardie, Claire},
 booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing},
 pages={172--182},
 year={2011},
 organization={Association for Computational Linguistics}
}

@article{grefenstette2013multi,
 title={Multi-step regression learning for compositional distributional semantics},
 author={Grefenstette, Edward and Dinu, Georgiana and Zhang, Yao-Zhong and Sadrzadeh, Mehrnoosh and Baroni, Marco},
 journal={arXiv preprint arXiv:1301.6939},
 year={2013}
}

@inproceedings{mikolov2013distributed,
 title={Distributed representations of words and phrases and their compositionality},
 author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle={Advances in neural information processing systems},
 pages={3111--3119},
 year={2013}
}

@inproceedings{socher2011dynamic,
 title={Dynamic pooling and unfolding recursive autoencoders for paraphrase detection},
 author={Socher, Richard and Huang, Eric H and Pennin, Jeffrey and Manning, Christopher D and Ng, Andrew Y},
 booktitle={Advances in Neural Information Processing Systems},
 pages={801--809},
 year={2011}
}

@article{mitra2016dual,
 title={A dual embedding space model for document ranking},
 author={Mitra, Bhaskar and Nalisnick, Eric and Craswell, Nick and Caruana, Rich},
 journal={arXiv preprint arXiv:1602.01137},
 year={2016}
}


@article{blei2003latent,
 title={Latent dirichlet allocation},
 author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
 journal={Journal of machine Learning research},
 volume={3},
 number={Jan},
 pages={993--1022},
 year={2003}
}


@inproceedings{ghosh2016characterizing,
 title={Characterizing diseases from unstructured text: A vocabulary driven word2vec approach},
 author={Ghosh, Saurav and Chakraborty, Prithwish and Cohn, Emily and Brownstein, John S and Ramakrishnan, Naren},
 booktitle={Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
 pages={1129--1138},
 year={2016},
 organization={ACM}
}

@article{joulin2016bag,
 title={Bag of tricks for efficient text classification},
 author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
 journal={arXiv preprint arXiv:1607.01759},
 year={2016}
}

@article{sidorov2014soft,
 title={Soft similarity and soft cosine measure: Similarity of features in vector space model},
 author={Sidorov, Grigori and Gelbukh, Alexander and G{\'o}mez-Adorno, Helena and Pinto, David},
 journal={Computaci{\'o}n y Sistemas},
 volume={18},
 number={3},
 pages={491--504},
 year={2014},
 publisher={Centro de Investigaci{\'o}n en Computaci{\'o}n, IPN}
}

@inproceedings{kusner2015word,
 title={From word embeddings to document distances},
 author={Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
 booktitle={International Conference on Machine Learning},
 pages={957--966},
 year={2015}
}


@inproceedings{rehurek_lrec,
 title = {{Software Framework for Topic Modelling with Large Corpora}},
 author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
 booktitle = {{Proceedings of the LREC 2010 Workshop on New
   Challenges for NLP Frameworks}},
 pages = {45--50},
 year = 2010,
 month = May,
 day = 22,
 publisher = {ELRA},
 address = {Valletta, Malta},
 note={\url{http://is.muni.cz/publication/884893/en}},
 language={English}
}

